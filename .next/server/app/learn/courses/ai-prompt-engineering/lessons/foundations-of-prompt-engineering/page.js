(()=>{var e={};e.id=886,e.ids=[886],e.modules={3295:e=>{"use strict";e.exports=require("next/dist/server/app-render/after-task-async-storage.external.js")},10846:e=>{"use strict";e.exports=require("next/dist/compiled/next-server/app-page.runtime.prod.js")},19121:e=>{"use strict";e.exports=require("next/dist/server/app-render/action-async-storage.external.js")},29294:e=>{"use strict";e.exports=require("next/dist/server/app-render/work-async-storage.external.js")},29325:(e,t,r)=>{"use strict";r.r(t),r.d(t,{GlobalError:()=>o.default,__next_app__:()=>g,pages:()=>d,routeModule:()=>c,tree:()=>l});var s=r(65239),n=r(48088),o=r(46076),a=r(30893),i={};for(let e in a)0>["default","tree","pages","GlobalError","__next_app__","routeModule"].indexOf(e)&&(i[e]=()=>a[e]);r.d(t,i);let l={children:["",{children:["learn",{children:["courses",{children:["ai-prompt-engineering",{children:["lessons",{children:["foundations-of-prompt-engineering",{children:["__PAGE__",{},{page:[()=>Promise.resolve().then(r.bind(r,95517)),"/Volumes/My Book/ Projects/React_Next/clouddogg_mcp/app/learn/courses/ai-prompt-engineering/lessons/foundations-of-prompt-engineering/page.tsx"]}]},{}]},{}]},{}]},{}]},{metadata:{icon:[async e=>(await Promise.resolve().then(r.bind(r,46055))).default(e)],apple:[],openGraph:[],twitter:[],manifest:void 0}}]},{layout:[()=>Promise.resolve().then(r.bind(r,77112)),"/Volumes/My Book/ Projects/React_Next/clouddogg_mcp/app/layout.tsx"],error:[()=>Promise.resolve().then(r.bind(r,52608)),"/Volumes/My Book/ Projects/React_Next/clouddogg_mcp/app/error.tsx"],"global-error":[()=>Promise.resolve().then(r.bind(r,46076)),"/Volumes/My Book/ Projects/React_Next/clouddogg_mcp/app/global-error.tsx"],"not-found":[()=>Promise.resolve().then(r.t.bind(r,57398,23)),"next/dist/client/components/not-found-error"],forbidden:[()=>Promise.resolve().then(r.t.bind(r,89999,23)),"next/dist/client/components/forbidden-error"],unauthorized:[()=>Promise.resolve().then(r.t.bind(r,65284,23)),"next/dist/client/components/unauthorized-error"],metadata:{icon:[async e=>(await Promise.resolve().then(r.bind(r,46055))).default(e)],apple:[],openGraph:[],twitter:[],manifest:void 0}}]}.children,d=["/Volumes/My Book/ Projects/React_Next/clouddogg_mcp/app/learn/courses/ai-prompt-engineering/lessons/foundations-of-prompt-engineering/page.tsx"],g={require:r,loadChunk:()=>Promise.resolve()},c=new s.AppPageRouteModule({definition:{kind:n.RouteKind.APP_PAGE,page:"/learn/courses/ai-prompt-engineering/lessons/foundations-of-prompt-engineering/page",pathname:"/learn/courses/ai-prompt-engineering/lessons/foundations-of-prompt-engineering",bundlePath:"",filename:"",appPaths:[]},userland:{loaderTree:l}})},33873:e=>{"use strict";e.exports=require("path")},46055:(e,t,r)=>{"use strict";r.r(t),r.d(t,{default:()=>n});var s=r(31658);let n=async e=>[{type:"image/x-icon",sizes:"16x16",url:(0,s.fillMetadataSegment)(".",await e.params,"favicon.ico")+""}]},63033:e=>{"use strict";e.exports=require("next/dist/server/app-render/work-unit-async-storage.external.js")},78335:()=>{},79551:e=>{"use strict";e.exports=require("url")},95517:(e,t,r)=>{"use strict";r.r(t),r.d(t,{default:()=>o,metadata:()=>n});var s=r(37413);let n={title:"Foundations of Prompt Engineering - AI Prompt Engineering - CloudDogg MCP Learning",description:"Learn the fundamentals of prompt engineering and how language models interpret prompts."};function o(){return(0,s.jsx)("div",{className:"min-h-screen bg-gradient-to-b from-gray-950 to-gray-900 text-gray-100 dark:bg-gradient-to-b dark:from-gray-950 dark:to-gray-900 dark:text-gray-100 light:bg-gradient-to-b light:from-blue-50 light:to-white light:text-gray-800",children:(0,s.jsxs)("div",{className:"relative overflow-hidden",children:[(0,s.jsx)("div",{className:"absolute top-0 left-1/4 w-96 h-96 bg-green-500/10 rounded-full blur-3xl dark:bg-green-500/10 light:bg-green-500/5"}),(0,s.jsx)("div",{className:"absolute bottom-1/3 right-1/4 w-64 h-64 bg-emerald-500/10 rounded-full blur-3xl dark:bg-emerald-500/10 light:bg-emerald-500/5"}),(0,s.jsx)("main",{className:"pt-32 pb-20 px-4 md:px-8",children:(0,s.jsx)("div",{className:"max-w-7xl mx-auto",children:(0,s.jsxs)("div",{className:"flex flex-col lg:flex-row gap-8",children:[(0,s.jsxs)("div",{className:"lg:w-3/4",children:[(0,s.jsxs)("div",{className:"mb-8",children:[(0,s.jsx)("h1",{className:"text-3xl md:text-4xl font-bold mb-4 bg-gradient-to-r from-green-400 via-emerald-300 to-teal-400 bg-clip-text text-transparent dark:from-green-400 dark:via-emerald-300 dark:to-teal-400 light:from-green-600 light:via-emerald-500 light:to-teal-600",children:"Module 1: Foundations of Prompt Engineering"}),(0,s.jsx)("p",{className:"text-xl text-gray-300 light:text-gray-700",children:"Understanding how AI models interpret prompts"})]}),(0,s.jsxs)("div",{className:"bg-gray-800/50 rounded-xl border border-gray-700 p-6 mb-8 light:bg-white light:border-gray-200 light:shadow-md",children:[(0,s.jsx)("div",{className:"aspect-w-16 aspect-h-9 mb-6",children:(0,s.jsx)("div",{className:"w-full h-full bg-gray-900 rounded-lg flex items-center justify-center light:bg-gray-100",children:(0,s.jsxs)("div",{className:"text-center",children:[(0,s.jsx)("div",{className:"w-16 h-16 mx-auto mb-4 rounded-full bg-gradient-to-r from-green-500 to-emerald-600 flex items-center justify-center",children:(0,s.jsxs)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-8 w-8 text-white",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor",children:[(0,s.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z"}),(0,s.jsx)("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M21 12a9 9 0 11-18 0 9 9 0 0118 0z"})]})}),(0,s.jsx)("p",{className:"text-gray-400 light:text-gray-600",children:"Video player will appear here"})]})})}),(0,s.jsx)("h2",{className:"text-2xl font-bold mb-4 text-white light:text-gray-800",children:"How Language Models Work"}),(0,s.jsxs)("div",{className:"prose prose-invert max-w-none light:prose-light",children:[(0,s.jsx)("p",{children:"Large Language Models (LLMs) like GPT-4, Claude, and others are based on transformer architectures that process text as sequences of tokens. Understanding how these models work is essential for effective prompt engineering."}),(0,s.jsx)("h3",{children:"Key Concepts in Language Models"}),(0,s.jsxs)("ul",{children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Tokens"}),": LLMs process text as tokens, which can be words, parts of words, or individual characters. The model has a context window that limits how many tokens it can process at once."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Training Data"}),": LLMs are trained on vast datasets of text from the internet, books, and other sources. This training data shapes what the model knows and how it responds."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Prediction"}),": At its core, an LLM predicts the next token in a sequence based on the tokens that came before it. This simple mechanism allows it to generate coherent text."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Context Window"}),': This is the number of tokens the model can "see" at once. For example, GPT-4 has a context window of up to 32,000 tokens, while earlier models had much smaller windows.']})]}),(0,s.jsx)("h3",{children:"How Prompts Influence Model Behavior"}),(0,s.jsx)("p",{children:"When you provide a prompt to an LLM, you're essentially giving it a starting point for its text generation. The model will attempt to continue the text in a way that's consistent with both its training data and the specific context you've provided."}),(0,s.jsx)("p",{children:"This means that the way you phrase your prompt has a significant impact on the response you'll receive. A well-crafted prompt provides:"}),(0,s.jsxs)("ul",{children:[(0,s.jsx)("li",{children:"Clear instructions about what you want"}),(0,s.jsx)("li",{children:"Necessary context for the model to understand the task"}),(0,s.jsx)("li",{children:"Examples or demonstrations if needed (few-shot learning)"}),(0,s.jsx)("li",{children:"Constraints or guidelines for the response format"})]}),(0,s.jsx)("h3",{children:"Model Limitations"}),(0,s.jsx)("p",{children:"Understanding the limitations of LLMs is crucial for effective prompt engineering:"}),(0,s.jsxs)("ul",{children:[(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Knowledge Cutoff"}),": Models have a training cutoff date and don't know about events after that date."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Hallucinations"}),": Models can generate plausible-sounding but incorrect information."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Reasoning Limitations"}),": Complex logical reasoning or mathematical calculations can be challenging."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)("strong",{children:"Bias"}),": Models can reflect biases present in their training data."]})]}),(0,s.jsx)("p",{children:"In the next lesson, we'll explore the anatomy of an effective prompt and how to structure your prompts for optimal results."})]})]}),(0,s.jsxs)("div",{className:"flex justify-between",children:[(0,s.jsx)("div",{})," ",(0,s.jsxs)("button",{className:"px-4 py-2 bg-gradient-to-r from-green-500 to-emerald-600 hover:from-green-600 hover:to-emerald-700 text-white rounded-lg flex items-center",children:["Next Lesson: The Anatomy of an Effective Prompt",(0,s.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",className:"h-5 w-5 ml-2",viewBox:"0 0 20 20",fill:"currentColor",children:(0,s.jsx)("path",{fillRule:"evenodd",d:"M10.293 5.293a1 1 0 011.414 0l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414-1.414L12.586 11H5a1 1 0 110-2h7.586l-2.293-2.293a1 1 0 010-1.414z",clipRule:"evenodd"})})]})]})]}),(0,s.jsx)("div",{className:"lg:w-1/4",children:(0,s.jsxs)("div",{className:"bg-gray-800/50 rounded-xl border border-gray-700 p-6 sticky top-32 light:bg-white light:border-gray-200 light:shadow-md",children:[(0,s.jsx)("h3",{className:"text-xl font-bold mb-4 text-white light:text-gray-800",children:"Module 1 Lessons"}),(0,s.jsxs)("ul",{className:"space-y-3",children:[(0,s.jsx)("li",{className:"p-3 bg-emerald-500/20 text-emerald-300 rounded-lg font-medium light:bg-emerald-100 light:text-emerald-700",children:"1.1 How Language Models Work"}),(0,s.jsx)("li",{className:"p-3 bg-gray-700/50 rounded-lg text-gray-300 hover:bg-gray-700 transition-colors light:bg-gray-100 light:text-gray-700 light:hover:bg-gray-200",children:"1.2 The Anatomy of an Effective Prompt"}),(0,s.jsx)("li",{className:"p-3 bg-gray-700/50 rounded-lg text-gray-300 hover:bg-gray-700 transition-colors light:bg-gray-100 light:text-gray-700 light:hover:bg-gray-200",children:"1.3 Common Prompt Engineering Mistakes"}),(0,s.jsx)("li",{className:"p-3 bg-gray-700/50 rounded-lg text-gray-300 hover:bg-gray-700 transition-colors light:bg-gray-100 light:text-gray-700 light:hover:bg-gray-200",children:"1.4 Hands-on: Prompt Analysis"}),(0,s.jsx)("li",{className:"p-3 bg-gray-700/50 rounded-lg text-gray-300 hover:bg-gray-700 transition-colors light:bg-gray-100 light:text-gray-700 light:hover:bg-gray-200",children:"1.5 Module 1 Quiz"})]}),(0,s.jsxs)("div",{className:"mt-6 pt-6 border-t border-gray-700 light:border-gray-200",children:[(0,s.jsx)("h4",{className:"font-medium mb-2 text-white light:text-gray-800",children:"Your Progress"}),(0,s.jsx)("div",{className:"w-full bg-gray-700 rounded-full h-2.5 mb-2 light:bg-gray-200",children:(0,s.jsx)("div",{className:"bg-gradient-to-r from-green-500 to-emerald-600 h-2.5 rounded-full",style:{width:"20%"}})}),(0,s.jsx)("p",{className:"text-sm text-gray-400 light:text-gray-600",children:"1 of 5 lessons completed"})]})]})})]})})})]})})}},96487:()=>{}};var t=require("../../../../../../webpack-runtime.js");t.C(e);var r=e=>t(t.s=e),s=t.X(0,[447,706,658,442],()=>r(29325));module.exports=s})();